{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCJu52qn-wp5",
    "outputId": "be52bc44-986f-49e5-83ba-ef08d71f3a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchao in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchtune in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.3.0)\n",
      "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.28.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.2)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.8)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.9.0)\n",
      "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtune) (1.26.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (6.0.0)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (10.2.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.21.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (2.2.3)\n",
      "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.3.0)\n",
      "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.11.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->torchtune) (4.9.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.8.30)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchao\n",
    "!pip install wandb\n",
    "!pip install torchtune\n",
    "# import wandb\n",
    "!pip install transformers\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKmediY1-18d",
    "outputId": "f65523f2-8c26-41a3-8ae1-ebd45c23f7fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "6d18c669de02412a959b20eaa818e3a6",
      "a932ec56d10b426e974d55e765a9f7c9",
      "feecc9d1f2ee4667beef28b2c5d607d2",
      "ec03a4e8461d4a1880daf1f307e172ff",
      "76d88d1671d64fc2afe2e17098045935",
      "14585b2d8e4d490abf54b0166c55fc9e",
      "c574aaeba1c74552b2346f034de152df",
      "d6d0c9dace824b5aada1c2af5ec08880",
      "e881780f23164737aabe9ce21d3a1a63",
      "c802c552ec1a40cf9213ca3b11c7af86",
      "281d510fcef84d999e25c7f1d726cdd7",
      "456abe4d692b4e068fb0f7c5af26fe55",
      "57e77de389bf4ca28967421a92db7544",
      "3257101601ef43e9949202e9496b893e",
      "229667119c3f40f2a01cace6f968ac14",
      "006ef0c5df0c40f3a8ab486a67e2a27c",
      "4ba9455baefa436d9e79eaabc88d2fdb",
      "011b5225e1184285be3b637af01aebce",
      "9a18e10dd6b94d44ac91610cb6feb453",
      "7b198fe21d104ddabe608174c9ddd681",
      "532dfbe8af8a4f69867b206493355e6e",
      "da0f9205f4114382b65b307d54eac834"
     ]
    },
    "id": "rDY24h9c_d6z",
    "outputId": "e8714fc0-4b34-4942-f0b2-d5edefda268b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3b7bc2637549da803c3eb8db85b43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/43.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebde02a5a2f541918e027e817af6cd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da277c25722e4d5ba4e15bb8ddf37fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "000_00000.parquet:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# use name=\"sample-10BT\" to use the 10BT sample\n",
    "fw_train = load_dataset(\"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=False)\n",
    "\n",
    "train_dataset = fw_train.train_test_split(test_size=0.2)\n",
    "# val_dataset = train_dataset['test']['text']\n",
    "# train_dataset = train_dataset['train']['text']\n",
    "\n",
    "# # Print the sizes of the splits\n",
    "# print(f\"Train dataset size: {len(list(train_dataset))}\")  # Convert to list to get the size\n",
    "# print(f\"Test dataset size: {len(list(val_dataset))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Z6fs5r3CUq7",
    "outputId": "fa3873ed-287e-44bc-f34a-ae20f3ba1fc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],\n",
       "        num_rows: 11895089\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],\n",
       "        num_rows: 2973773\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5847f384b382459c8596f17a4d3ff94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13eda4fb518d4dbbbded02fac5e54ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31363ec5b3c34cb59e58f1dbc5d42756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer saved to 'bpe_tokenizer_30k.json'\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm  # For progress bar\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=False)\n",
    "dataset = dataset.select(range(1000000))\n",
    "\n",
    "train_texts = dataset['text']\n",
    "# test_texts = dataset['test']['text']\n",
    "\n",
    "# Initialize a tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Create a BpeTrainer with your desired vocabulary size\n",
    "vocab_size = 30000  # Adjust the vocabulary size here\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "# Define a generator to yield batches of text with progress tracking\n",
    "def batch_iterator(texts, batch_size=1000):\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        yield texts[i:i + batch_size]\n",
    "\n",
    "# Train the tokenizer using batching with progress tracking\n",
    "tokenizer.train_from_iterator(\n",
    "    batch_iterator(train_texts, batch_size=1000),  # Adjust batch size as needed\n",
    "    trainer=trainer,\n",
    "    length=len(train_texts)  # Total number of texts\n",
    ")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"bpe_tokenizer_30k.json\")\n",
    "print(\"Tokenizer saved to 'bpe_tokenizer_30k.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],\n",
       "    num_rows: 1000000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [19480, 16, 10647, 5]\n",
      "Decoded: Hello , world !\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer_30k.json\")\n",
    "\n",
    "# Encode and decode functions\n",
    "encode = lambda s: tokenizer.encode(s).ids\n",
    "decode = lambda l: tokenizer.decode(l)\n",
    "\n",
    "# Example usage\n",
    "encoded = encode(\"Hello, world!\")\n",
    "decoded = decode(encoded)\n",
    "print(f\"Encoded: {encoded}\\nDecoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lrLGC8c8ATfO"
   },
   "outputs": [],
   "source": [
    "# #BPE Training notebook from https://github.com/Infatoshi/fcc-intro-to-llms\n",
    "\n",
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import BPE\n",
    "# from tokenizers.trainers import BpeTrainer\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "\n",
    "# # Extract text data from the datasets\n",
    "# def extract_text(dataset):\n",
    "#     return [example[\"text\"] for example in dataset]\n",
    "\n",
    "# train_texts = extract_text(train_dataset['train']['text'])\n",
    "# val_texts = extract_text(train_dataset['train']['text'])\n",
    "\n",
    "# # Combine train and validation texts\n",
    "# all_texts = train_texts + val_texts\n",
    "\n",
    "\n",
    "# # Initialize a tokenizer with BPE model\n",
    "# tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# # Create a BpeTrainer with your desired vocabulary size\n",
    "# vocab_size = 30000  # Adjust the vocabulary size here\n",
    "# trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=vocab_size)\n",
    "\n",
    "\n",
    "# # Train the tokenizer\n",
    "\n",
    "# tokenizer.train(all_texts, trainer)\n",
    "\n",
    "# # Save the tokenizer\n",
    "# tokenizer.save(\"bpe_tokenizer_30k.json\")\n",
    "\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = Tokenizer.from_file(\"bpe_tokenizer_30k.json\")\n",
    "\n",
    "# # Encode and decode functions\n",
    "# encode = lambda s: tokenizer.encode(s).ids\n",
    "# decode = lambda l: tokenizer.decode(l)\n",
    "\n",
    "# # Example usage\n",
    "# encoded = encode(\"Hello, world!\")\n",
    "# decoded = decode(encoded)\n",
    "# print(f\"Encoded: {encoded}\\nDecoded: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 14 12:14:53 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   39C    P2             58W /  450W |   16772MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  |   00000000:61:00.0 Off |                  Off |\n",
      "|  0%   39C    P2             63W /  450W |   16772MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4090        On  |   00000000:81:00.0 Off |                  Off |\n",
      "|  0%   37C    P2             55W /  450W |   16772MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4090        On  |   00000000:A1:00.0 Off |                  Off |\n",
      "|  0%   35C    P2             56W /  450W |   16772MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('[PAD]').ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qJqCz3EiJmcm"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f635420ede949608983c4cdbe242544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b62fcf2e0d443c78379e65df1ea2923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c99dd089d7b4c5eb18e72c32b2a86e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae122d3651a84fc690d4897f7977d337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58992ecbf7946179f4cb0f1ba0ccb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# from google.colab import userdata\n",
    "# HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", hf_token = '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')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "1638400\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data['input_ids']))\n",
    "# print(train_data['input_ids'][0])\n",
    "\n",
    "train_data_flat = [token_id for seq in train_data['input_ids'] for token_id in seq]\n",
    "val_data_flat = [token_id for seq in val_data['input_ids'] for token_id in seq]\n",
    "\n",
    "train_data_flat_tensor = torch.tensor(train_data_flat, dtype=torch.long)\n",
    "print(len(train_data_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, split):\n",
    "    block_size = ModelArgs.block_size\n",
    "    batch_size = len(batch)\n",
    "    if(split == 'train'):\n",
    "        data = train_data_tensor\n",
    "    elif(split == 'test'):\n",
    "        data = val_data_tensor\n",
    "    ix = torch.randint(len(data) - ModelArgs.block_size, (ModelArgs.batch_size,))\n",
    "    x = torch.stack([data[i:i+ModelArgs.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ModelArgs.block_size+1] for i in ix])\n",
    "\n",
    "    print(\"Shape of x: \", len(x))\n",
    "    print(\"Length of y: \", len(y))\n",
    "    # x, y = x.to(ModelArgs.device), y.to(ModelArgs.device)\n",
    "    # x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    # y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    # for i, sequence in enumerate(batch):\n",
    "    #     print(\"Seq: \", sequence)\n",
    "    #     print(\"Shape x: \", sequence[:-1].shape)\n",
    "    #     print(\"Shape of y: \", len(sequence[1:]))\n",
    "    #     x[i] = sequence[:-1]  # Input is all tokens except the last one\n",
    "    #     y[i] = sequence[1:]   # Target is all tokens except the first one\n",
    "    return x, y\n",
    "    \n",
    "\n",
    "# train_data = create_sequences(train_data_flat['input_ids'], ModelArgs.block_size)\n",
    "# val_data = create_sequences(val_data['input_ids'], ModelArgs.block_size)\n",
    "\n",
    "\n",
    "# Define collate_fn\n",
    "def collate_fn(split, batch):\n",
    "    block_size = ModelArgs.block_size\n",
    "    batch_size = len(batch)\n",
    "    data = None\n",
    "    if(split == 'train'):\n",
    "        data = train_data_flat_tensor\n",
    "    elif(split == 'test'):\n",
    "        data = val_data_flat_tensor\n",
    "    ix = torch.randint(len(data) - ModelArgs.block_size, (ModelArgs.batch_size,))\n",
    "    x = torch.stack([data[i:i+ModelArgs.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ModelArgs.block_size+1] for i in ix])\n",
    "\n",
    "    print(\"Shape of x: \", len(x))\n",
    "    print(\"Length of y: \", len(y))\n",
    "    # x, y = x.to(ModelArgs.device), y.to(ModelArgs.device)\n",
    "    # x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    # y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    # for i, sequence in enumerate(batch):\n",
    "    #     print(\"Seq: \", sequence)\n",
    "    #     print(\"Shape x: \", sequence[:-1].shape)\n",
    "    #     print(\"Shape of y: \", len(sequence[1:]))\n",
    "    #     x[i] = sequence[:-1]  # Input is all tokens except the last one\n",
    "    #     y[i] = sequence[1:]   # Target is all tokens except the first one\n",
    "    return x, y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x:  4\n",
      "Length of y:  4\n",
      "(tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]))\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "train_loader = DataLoader(train_tokens, batch_size=ModelArgs.batch_size, shuffle=True, collate_fn= partial(collate_fn, 'train'))\n",
    "val_loader = DataLoader(val_tokens, batch_size=ModelArgs.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# print(train_loader)\n",
    "print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9b7d94ba15da4969b13658501c044be5",
      "bf6b831ddf284f97a7d5cbdc251b5546",
      "5fdef1073deb44f49c75e5a4d4dbe3b1",
      "e6970785434b40e5b31690b6c0221a77",
      "769329623b944cb69a2b163a555f3fc4",
      "130d4aebf9fe4c5b8119c6647b10342f",
      "0190bc823cfa4f569673d0e38a0a0a9e",
      "c33bfb3f974f411fa1d20fcdd557052d",
      "04784be0738f4ba69a4b4dd44b931711",
      "6079efb227a7455a9f76a338c0b52282",
      "3b6f04a4e03d4545ba1d030bf130f7e7"
     ]
    },
    "id": "DcrOJq5D6_oT",
    "outputId": "4ea6244d-3f8c-4917-d8f2-39578d56566d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8615dfb6664873ac8bc779f51ca3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2bcf6cdca9491997c9652e60adbc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3576267207c4a1b8cc40e804f8a7c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bfdedd14f24b75ac4ab645a2241525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column tokens not in the dataset. Current columns in the dataset: ['text', 'input_ids', 'attention_mask']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 208\u001b[0m\n\u001b[1;32m    199\u001b[0m val_data \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8000\u001b[39m, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdump\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m'\u001b[39m], num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# print(train_data['tokens'])\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# # Extract tokens from the processed datasets\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# train_tokens = train_data['tokens']\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# val_tokens = val_data['tokens']\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Flatten the tokenized data\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m train_tokens \u001b[38;5;241m=\u001b[39m [token_id \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m seq]\n\u001b[1;32m    209\u001b[0m val_tokens \u001b[38;5;241m=\u001b[39m [token_id \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m val_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m seq]\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Convert to tensors\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# train_data = torch.tensor(train_data['input_ids'], dtype=torch.long)\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# val_data = torch.tensor(val_data['input_ids'], dtype=torch.long)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# # Debug output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py:2780\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py:2764\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2762\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2764\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2765\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2766\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2767\u001b[0m )\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py:590\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    588\u001b[0m         _raise_bad_key_type(key)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 590\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py:527\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 527\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column tokens not in the dataset. Current columns in the dataset: ['text', 'input_ids', 'attention_mask']\""
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "# from torchtune.modules import RMSNorm\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "# use name=\"sample-10BT\" to use the 10BT sample\n",
    "fw_train = load_dataset(\"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=False)\n",
    "\n",
    "# Select only 1000 rows from the dataset\n",
    "fw_train = fw_train.select(range(1000))\n",
    "\n",
    "# print(fw_train)\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset = fw_train.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "# Access the splits\n",
    "# train_dataset = train_val_split['train']\n",
    "# val_dataset = train_val_split['test']\n",
    "\n",
    "# train_dataset = fw_train.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "def setup(rank=None, world_size=None):\n",
    "    # os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    # os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(\"nccl\")\n",
    "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
    "def cleanup():\n",
    "    destroy_process_group()\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    #Hyperparameters\n",
    "\n",
    "    block_size = 128\n",
    "    batch_size = 4\n",
    "    embeddings_dims = 786\n",
    "    attn_dropout = 0.1\n",
    "    no_of_heads = 6 #IMP needs to be thoroughly calculated\n",
    "    dropout = 0.1\n",
    "    # epochs = 100\n",
    "    max_lr = 2e-4\n",
    "    no_of_decoder_layers = 6 #IMP needs to be thoroughly calculated\n",
    "    weight_decay_optim = 0.01\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.95\n",
    "    clip = 1.0\n",
    "    device = 'cuda'\n",
    "    no_kv_heads = 2\n",
    "    vocab_size = 30000\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "data_path = Path('data')\n",
    "data_path.mkdir(exist_ok=True)\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# !cp input.txt data/input.txt\n",
    "\n",
    "\n",
    "\n",
    "#Datasets\n",
    "\n",
    "# Using tinyshakespeare\n",
    "\n",
    "# with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "# tokenizer = Tokenizer.from_file(\"bpe_tokenizer_30k.json\")\n",
    "\n",
    "# Encode and decode functions\n",
    "# encode = lambda s: tokenizer.encode(s).ids\n",
    "# decode = lambda l: tokenizer.decode(l)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(model):\n",
    "    ckp = model.module.state_dict()\n",
    "    torch.save(ckp, \"datacheckpoint.pt\")\n",
    "    print(\"Checkpoint saved\")\n",
    "\n",
    "\n",
    "\n",
    "#Subword level tokenization\n",
    "\n",
    "#Loading custom trained BPE\n",
    "# Load the tokenizer\n",
    "# tokenizer = Tokenizer.from_file(\"data/bpe_tokenizer_tinyshakespeare_1k.json\")\n",
    "# vocab_size = tokenizer.get_vocab_size()\n",
    "# Encode and decode functions\n",
    "# encode = lambda s: tokenizer.encode(s).ids\n",
    "# decode = lambda l: tokenizer.decode(l)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#Character level tokenization\n",
    "\n",
    "# # here are all the unique characters that occur in this text\n",
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "\n",
    "\n",
    "# # create a mapping from characters to integers\n",
    "# stoi = { ch: i for i,ch in enumerate(chars) }\n",
    "# itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "# Convert the dataset to Hugging Face Dataset format\n",
    "# train_hf_dataset = Dataset.from_dict({\"text\": train_dataset['train']['text']})\n",
    "# val_hf_dataset = Dataset.from_dict({\"text\": train_dataset['test']['text']})\n",
    "\n",
    "# Tokenize the dataset using the `map` function\n",
    "\n",
    "\n",
    "# from google.colab import userdata\n",
    "# HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", hf_token = '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')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=2048,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "## Load the tokenizer\n",
    "# tokenizer = Tokenizer.from_file(\"bpe_tokenizer_30k.json\")\n",
    "\n",
    "# Tokenization functions\n",
    "\n",
    "def encode(examples):\n",
    "    tokens = []\n",
    "    for example in examples['text']:\n",
    "        out = tokenizer.encode(example).ids\n",
    "        tokens.append(out)  # Append the tokenized sequence (do not flatten)\n",
    "    return {\"tokens\": tokens}\n",
    "\n",
    "\n",
    "# def encode_train(examples):\n",
    "#     tokens = []\n",
    "#     for example in examples['text']:\n",
    "#         out = tokenizer.encode(example).ids\n",
    "#         tokens.append(out)  # Append the tokenized sequence (do not flatten)\n",
    "#     return {\"tokens\": tokens}\n",
    "\n",
    "# def encode_val(examples):\n",
    "#     tokens = []\n",
    "#     for example in examples['text']:\n",
    "#         out = tokenizer.encode(example).ids\n",
    "#         tokens.append(out)  # Append the tokenized sequence (do not flatten)\n",
    "#     return {\"tokens\": tokens}\n",
    "\n",
    "# Apply tokenization with batching\n",
    "train_data = train_dataset['train'].map(tokenize_function, batched=True, batch_size=8000, remove_columns=['id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'], num_proc=8)\n",
    "val_data = train_dataset['test'].map(tokenize_function, batched=True, batch_size=8000, remove_columns=['id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'], num_proc=8)\n",
    "\n",
    "\n",
    "# print(train_data['tokens'])\n",
    "# # Extract tokens from the processed datasets\n",
    "# train_tokens = train_data['tokens']\n",
    "# val_tokens = val_data['tokens']\n",
    "\n",
    "# Flatten the tokenized data\n",
    "train_tokens = [token_id for seq in train_data['tokens'] for token_id in seq]\n",
    "val_tokens = [token_id for seq in val_data['tokens'] for token_id in seq]\n",
    "\n",
    "# Convert to tensors\n",
    "# train_data = torch.tensor(train_data['input_ids'], dtype=torch.long)\n",
    "# val_data = torch.tensor(val_data['input_ids'], dtype=torch.long)\n",
    "\n",
    "# # Debug output\n",
    "print(\"Number of train tokens:\", len(train_data))\n",
    "print(\"Number of validation tokens:\", len(val_data))\n",
    "\n",
    "\n",
    "def create_sequences(data, block_size):\n",
    "    sequences = []\n",
    "\n",
    "    for seq in data:\n",
    "        if len(seq) < block_size:\n",
    "            # while(len(sequence) < block_size):\n",
    "                # sequence = data[i:i + block_size + 1]\n",
    "           \n",
    "                # Pad the sequence if it's shorter than block_size\n",
    "            padding_length = block_size - len(seq)\n",
    "            seq = torch.cat([seq, torch.full((padding_length,), tokenizer.pad_token_id, dtype=torch.long)])\n",
    "        sequences.append(seq)\n",
    "    out = torch.tensor(sequences, dtype=torch.long)\n",
    "    return out\n",
    "\n",
    "# train_data = create_sequences(train_data['input_ids'], ModelArgs.block_size)\n",
    "# val_data = create_sequences(val_data['input_ids'], ModelArgs.block_size)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - ModelArgs.block_size, (ModelArgs.batch_size,))\n",
    "    x = torch.stack([data[i:i+ModelArgs.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ModelArgs.block_size+1] for i in ix])\n",
    "    x, y = x.to(ModelArgs.device), y.to(ModelArgs.device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self._data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self._data[idx:idx+self.block_size]\n",
    "        y = self._data[idx+1:idx+self.block_size+1]\n",
    "        return x, y\n",
    "\n",
    "# train_rows = 11895089\n",
    "# encoded_data = torch.tensor(encode(fw_train['text']), dtype=torch.long)\n",
    "# train_data = train_data[:train_rows]\n",
    "# val_data = val_data[train_rows:]\n",
    "\n",
    "# train_dataset = TextDataset(train_data, ModelArgs.block_size)\n",
    "# val_dataset = TextDataset(val_data, ModelArgs.block_size)\n",
    "\n",
    "# encoded_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# print(train_data)\n",
    "# print(val_data)\n",
    "# train_dataset = TextDataset(train_data, ModelArgs.block_size)\n",
    "# val_dataset = TextDataset(val_data, ModelArgs.block_size)\n",
    "\n",
    "# print(train_dataset)\n",
    "# print(val_dataset)\n",
    "\n",
    "\n",
    "# # Convert the tokenized data into a list of sequences\n",
    "# train_sequences = [train_data[i:i + ModelArgs.block_size] for i in range(0, len(train_data) - ModelArgs.block_size)]\n",
    "# val_sequences = [val_data[i:i + ModelArgs.block_size] for i in range(0, len(val_data) - ModelArgs.block_size)]\n",
    "\n",
    "# Define collate_fn\n",
    "def collate_fn(batch):\n",
    "    block_size = ModelArgs.block_size\n",
    "    batch_size = len(batch)\n",
    "    x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    for i, sequence in enumerate(batch):\n",
    "        print(\"Shape x: \", sequence[:-1].shape)\n",
    "        print(\"Shape of y: \", len(sequence[1:]))\n",
    "        x[i] = sequence[:-1]  # Input is all tokens except the last one\n",
    "        y[i] = sequence[1:]   # Target is all tokens except the first one\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rmsnorm_layer = torch.nn.RMSNorm(normalized_shape=embeddings_dims)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.rmsnorm_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "class RotaryEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "         device,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        batch_size: int = ModelArgs.batch_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings_dims = embeddings_dims\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.theta = 0\n",
    "\n",
    "\n",
    "    # def init_matrix(self, seq_len):\n",
    "    #         self.matrix = torch.zeros((seq_len, self.embeddings_dims, self.embeddings_dims), dtype=torch.float32,  requires_grad=False)\n",
    "    #         for pos in range(seq_len):\n",
    "    #             for j in range(1, self.embeddings_dims // 2):\n",
    "    #                 self.theta = 10000 ** (-2*(pos-1) / self.embeddings_dims)\n",
    "    #                 self.matrix[pos, 2*j + 1, 2*j + 1] = np.cos((pos*self.theta))\n",
    "    #                 self.matrix[pos, 2*j + 1, j + 1] = -np.sin((pos* self.theta))\n",
    "    #                 self.matrix[pos, 2*j , 2*j ] = -np.cos((pos* self.theta))\n",
    "    #                 self.matrix[pos, 2*j + 1, 2*j + 1] = np.sin((pos* self.theta))\n",
    "    #         return self.matrix\n",
    "        self.device=device\n",
    "\n",
    "    def init_matrix(self, seq_len):\n",
    "        self.matrix = torch.zeros((seq_len, self.embeddings_dims, self.embeddings_dims), dtype=torch.float32,  requires_grad=False,  device = self.device)\n",
    "\n",
    "        positions = torch.arange(seq_len,  dtype=torch.float32,  device = self.device).unsqueeze(1)\n",
    "        # dims = torch.arange(1, self.embeddings_dims // 2,  dtype=torch.float32)\n",
    "        theta = 10000 ** (-2 * (positions - 1) / self.embeddings_dims)\n",
    "        angles = positions * theta\n",
    "\n",
    "        cos_angles = torch.cos(angles)\n",
    "        sin_angles = torch.sin(angles)\n",
    "\n",
    "        indices = torch.arange(self.embeddings_dims,  dtype=torch.int64,  device = self.device)\n",
    "        # print(indices)\n",
    "        # print(indices.shape)\n",
    "        # print(indices[::2])\n",
    "        even_indices = indices[::2]\n",
    "        odd_indices = indices[1::2]\n",
    "\n",
    "        self.matrix[:, even_indices, even_indices] = cos_angles\n",
    "        self.matrix[:, odd_indices, odd_indices] = sin_angles\n",
    "        self.matrix[:, odd_indices, even_indices] = -sin_angles\n",
    "        self.matrix[:, even_indices, odd_indices] = cos_angles\n",
    "\n",
    "        return self.matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B,T,C = x.shape\n",
    "        # print(\"MATRIX:\",x)\n",
    "        if(x > self.block_size):\n",
    "            matrix = self.init_matrix(x)\n",
    "            return matrix\n",
    "        else:\n",
    "            matrix = self.init_matrix(self.block_size)\n",
    "\n",
    "            return matrix\n",
    "\n",
    "\n",
    "class RotaryAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "         device,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        no_of_heads: int = ModelArgs.no_of_heads,\n",
    "        attn_dropout: int = ModelArgs.attn_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.rotary_matrix = RotaryEmbeddings(embeddings_dims=embeddings_dims,  device = device)\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.device = device\n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "        query = self.query(x)\n",
    "        # print(query)\n",
    "        key = self.key(x)\n",
    "        values = self.value(x)\n",
    "        matrix = self.rotary_matrix(block_size)\n",
    "\n",
    "        # print(matrix.shape)\n",
    "        # print(query.shape)\n",
    "        masked = torch.tril(torch.ones((block_size, block_size),  requires_grad=False,  device = self.device))\n",
    "        rotary_query = matrix @ query.permute(1,2,0) # (B,T, C,C) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "        rotary_key = matrix @ key.permute(1,2,0)  #  (B,T, C,C  ) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "        weights = rotary_query.permute(2,0,1) @ rotary_key.permute(2,0,1).transpose(-2, -1)#(B,T,C,T) @ (B,T,C,T) = (T,C,C,T)\n",
    "        weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "        scaled_weights = weights_masked / (torch.sqrt(torch.tensor(key.shape[-1])))\n",
    "        scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "        value = scaled_weights @ values\n",
    "        out = self.dropout(value)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        no_of_kv_heads: int = ModelArgs.no_of_heads,\n",
    "        no_of_heads: int = ModelArgs.no_of_heads,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.no_of_kv_heads = no_of_kv_heads\n",
    "        self.no_of_q_heads = no_of_heads // no_of_kv_heads\n",
    "        self.head_size = embeddings_dims // self.no_of_q_heads\n",
    "        self.rotary_matrix = RotaryEmbeddings(embeddings_dims=embeddings_dims,  device = device)\n",
    "        # self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  bias=False)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  dtype=torch.float32, bias=False,  device = device)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  dtype=torch.float32, bias=False,  device = device)\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  dtype=torch.float32, bias=False,  device = device)\n",
    "        self.device = device\n",
    "        self.multi_query = nn.ModuleList([nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  bias=False,  device = self.device) for _ in range(self.no_of_q_heads)])\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, block_size, matrix):\n",
    "\n",
    "            # masked = torch.tril(torch.ones((block_size, block_size),  requires_grad=False,  device = self.device))\n",
    "\n",
    "            masked = torch.tril(torch.ones((block_size, block_size),  requires_grad=False,  device = self.device))\n",
    "            rotary_query = matrix @ q.permute(1,2,0) # (B,T, C,C) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "            rotary_key = matrix @ k.permute(1,2,0)  #  (B,T, C,C  ) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "            weights = rotary_query.permute(2,0,1) @ rotary_key.permute(2,0,1).transpose(-2, -1)#(B,T,C,T) @ (B,T,C,T) = (T,C,C,T)\n",
    "            weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "            scaled_weights = weights_masked / (torch.sqrt(torch.tensor(k.shape[-1])))\n",
    "            scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "            value = scaled_weights @ v\n",
    "            out = self.dropout(value)\n",
    "            return value\n",
    "\n",
    "    def forward(self,x):\n",
    "        # print(\"MQA: \", x.shape)\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "\n",
    "        # query = self.query(x)\n",
    "        matrix = self.rotary_matrix(block_size)\n",
    "\n",
    "\n",
    "        key = self.key(x)\n",
    "        values = self.value(x)\n",
    "\n",
    "        multi_query_concat = torch.cat([self.scaled_dot_product(query(x), key, values, block_size, matrix) for query in self.multi_query], dim=-1)\n",
    "\n",
    "\n",
    "        linear_layer= self.linear_layer(multi_query_concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "         device,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        no_of_q_heads: int = ModelArgs.no_of_heads,\n",
    "        no_of_kv_heads: int = ModelArgs.no_kv_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.no_of_kv_heads = no_of_kv_heads\n",
    "        self.no_of_q_heads = no_of_q_heads\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims * self.no_of_kv_heads, out_features=embeddings_dims , dtype=torch.float32,  bias=False,  device = device)\n",
    "        self.device = device\n",
    "        self.mqa = nn.ModuleList([MQA(embeddings_dims=embeddings_dims, device = self.device, block_size=block_size) for _ in range(self.no_of_kv_heads)])\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "\n",
    "\n",
    "        grouped_query_concat = torch.cat([group(x) for group in self.mqa], dim=-1)\n",
    "\n",
    "        linear_layer= self.linear_layer(grouped_query_concat)\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "         device,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * self.sig(x)\n",
    "\n",
    "        return swish\n",
    "\n",
    "\n",
    "\n",
    "class SWiGLU(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.swish = Swish(block_size=block_size, embeddings_dims=embeddings_dims, device=device)\n",
    "        self.linear_layer1 = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.linear_layer2 = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.linear_layer3 = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish_res = self.swish(self.linear_layer1(x))\n",
    "        x_V = self.linear_layer2(x)\n",
    "        res = torch.mul(swish_res, x_V)\n",
    "        out = self.linear_layer3(res)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self,\n",
    "                  device,\n",
    "                  embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                  block_size: int = ModelArgs.block_size,\n",
    "                  vocab_size: int = ModelArgs.vocab_size,\n",
    "                   dropout = ModelArgs.dropout\n",
    "\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  dtype=torch.float32,  device = device)\n",
    "        self.swiglue = SWiGLU(block_size=block_size, embeddings_dims=embeddings_dims,  device = device)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.swiglue(x)\n",
    "        x = self.linear_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                  device,\n",
    "                embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                dropout = ModelArgs.dropout,\n",
    "                block_size: int = ModelArgs.block_size,\n",
    "                vocab_size: int = ModelArgs.vocab_size,\n",
    "\n",
    "                 ) :\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.feedforward_network = FFN(embeddings_dims=embeddings_dims, block_size=block_size, vocab_size=vocab_size,  device = device)\n",
    "        self.gqa = GQA(embeddings_dims=embeddings_dims, block_size=block_size, no_of_kv_heads=ModelArgs.no_kv_heads, no_of_q_heads=ModelArgs.no_of_heads,  device = device)\n",
    "        # self.norm = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.norm1 = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.norm2 = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.norm1(x + self.gqa(x))\n",
    "        x = self.norm2(x + self.feedforward_network(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Llama(nn.Module):\n",
    "    def __init__(self,\n",
    "                device,\n",
    "                  embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                  no_of_decoder_layers: int = ModelArgs.no_of_decoder_layers,\n",
    "                  block_size: int = ModelArgs.block_size,\n",
    "                  vocab_size: int = ModelArgs.vocab_size,\n",
    "                  dropout = ModelArgs.dropout\n",
    "\n",
    "                 ) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dims,  dtype=torch.float32,  device = device)\n",
    "        self.decoder = nn.Sequential(*[DecoderLayer(embeddings_dims=embeddings_dims, block_size=block_size, vocab_size=vocab_size, dropout=dropout,  device = device) for _ in range(no_of_decoder_layers)])\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=vocab_size,  dtype=torch.float32,  device = device)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        # self.norm = Normalization(embeddings_dims)\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.decoder(x)\n",
    "        # x = self.norm(x)\n",
    "        x = self.linear_layer(x)\n",
    "        # out = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "# ModelArgs.device = device\n",
    "# model = Llama(device=ModelArgs.device, embeddings_dims=ModelArgs.embeddings_dims, block_size=ModelArgs.block_size, vocab_size=ModelArgs.vocab_size, dropout=ModelArgs.dropout)\n",
    "# model = model.to(ModelArgs.device)\n",
    "\n",
    "#Printing a summary of the architecture\n",
    "# !pip install torchinfo\n",
    "# from torchinfo import summary\n",
    "# idx, targets = get_batch('test')\n",
    "# # sample_idx = random.randint(range(len(train_dataset)))\n",
    "# # idx, targets = train_dataset[0]\n",
    "# idx = idx.to(ModelArgs.device)\n",
    "# # targets = targets.to(ModelArgs.device)\n",
    "# summary(model=model,\n",
    "#         input_data=idx,\n",
    "#         # input_size=(ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])\n",
    "\n",
    "\n",
    "def find_unused_parameters(model):\n",
    "    unused = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is None:\n",
    "            unused.append(name)\n",
    "    return unused\n",
    "\n",
    "\n",
    "\n",
    "#Train the  model\n",
    "\n",
    "def train():\n",
    "    setup()\n",
    "    device=torch.distributed.get_rank()\n",
    "\n",
    "\n",
    "    # rank = torch.distributed.get_rank()\n",
    "    print(f\"Start running basic DDP example on rank {device}.\")\n",
    "    # # create model and move it to GPU with id rank\n",
    "    # device_id = rank % torch.cuda.device_count()\n",
    "    # CFG = ModelArgs()\n",
    "\n",
    "    if(device == 0):\n",
    "        # Initialise run\n",
    "        wandb.init(\n",
    "            # entity = 'rajceo2031',\n",
    "                        project = 'Llama-DDP-Pretrain-10-billion-tokens',\n",
    "                        # config = CFG,\n",
    "                        # save_code = True,\n",
    "                        #group = 'ANN',\n",
    "                        #job_type = 'train'\n",
    ")\n",
    "\n",
    "    model = Llama(embeddings_dims=ModelArgs.embeddings_dims, block_size=ModelArgs.block_size, vocab_size=ModelArgs.vocab_size, dropout=ModelArgs.dropout, device=device)\n",
    "    # Optimizer setup and scheduler steup\n",
    "    torch.cuda.set_device(device)\n",
    "    model = model.cuda()\n",
    "\n",
    "    # Wrap model with DDP after moving to GPU\n",
    "    model = DDP(model, find_unused_parameters=False)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=ModelArgs.max_lr, betas=(ModelArgs.beta_1, ModelArgs.beta_2), weight_decay=ModelArgs.weight_decay_optim)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5000, T_mult=1, eta_min=0.001)\n",
    "\n",
    "    # optimizer = torch.optim.AdamW(params=model.parameters(), lr=ModelArgs.max_lr)\n",
    "    # Create DataLoader with collate_fn\n",
    "    train_loader = DataLoader(train_tokens, batch_size=ModelArgs.batch_size, shuffle=False, sampler=DistributedSampler(train_data), collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_tokens, batch_size=ModelArgs.batch_size, shuffle=False, sampler=DistributedSampler(val_data), collate_fn=collate_fn)\n",
    "        # print(train_loader)\n",
    "    print(next(iter(train_loader)))\n",
    "\n",
    "    save_chechpoint_iter = 1000\n",
    "    total_iters = 100000\n",
    "    eval_iters = 1000\n",
    "    # for X,y in train_loader:\n",
    "    #     print(X.shape)\n",
    "    #     print(y.shape)\n",
    "\n",
    "     # Only create progress bar for rank 0\n",
    "    # eval_epoch_iterator = range(eval_iters)\n",
    "    train_epoch_iterator = range(total_iters)\n",
    "    if device == 0:\n",
    "        train_epoch_iterator = tqdm(train_epoch_iterator, desc=\"Training\")\n",
    "        # eval_epoch_iterator = tqdm(eval_epoch_iterator, desc='Validation')\n",
    "\n",
    "    # lr_scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max= total_steps - initial_iters)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def estimate_loss():\n",
    "        out = {}\n",
    "\n",
    "        model.eval()\n",
    "        loader = None\n",
    "        # print(\"Starting the eval...\")\n",
    "        for split in ['train', 'val']:\n",
    "            print(f\"Starting with {split} evaluation...\")\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                # idx, targets = get_batch(split=split)\n",
    "                if(split == 'train'):\n",
    "                    loader = train_loader\n",
    "                else:\n",
    "                    loader = val_loader\n",
    "\n",
    "                # for idx, targets in loader:\n",
    "                idx, targets = next(iter(loader))\n",
    "                idx = idx.cuda(non_blocking=True)\n",
    "                targets = targets.cuda(non_blocking=True)\n",
    "                logits = model(idx)\n",
    "                batch_size, block_size, embeddings_dims = logits.shape\n",
    "                logits = logits.view(batch_size*block_size, embeddings_dims) # Total tokens(words) => batch_size * block_size\n",
    "                targets = targets.view(batch_size * block_size)\n",
    "                loss = nn.functional.cross_entropy(logits, targets)\n",
    "                losses[k] = loss.item()\n",
    "\n",
    "                # if device == 0:\n",
    "                #     eval_epoch_iterator.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "            out[split] = losses.mean()\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        return out\n",
    "\n",
    "    # model = model.to(rank)\n",
    "    model.train()\n",
    "\n",
    "    # for step in tqdm(range(total_iters)):\n",
    "    for step in train_epoch_iterator:\n",
    "        train_loader.sampler.set_epoch(step)\n",
    "        val_loader.sampler.set_epoch(step)\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if (step  % eval_iters == 0 and step != 0) or step == total_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            # print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            if device == 0:  # Only print on main process\n",
    "                print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "                # Log training loss more frequently\n",
    "        # if device == 0:\n",
    "                wandb.log({\n",
    "                    \"training_step_loss\": losses['train'],\n",
    "                    \"val_step_loss\": losses['val'],\n",
    "                    \"step\": step\n",
    "                })\n",
    "        if(step % save_chechpoint_iter == 0 and device == 0 and step != 0):\n",
    "            print(f\"Saving the model checkpoint for step: {step}\")\n",
    "            save_checkpoint(model)\n",
    "\n",
    "\n",
    "\n",
    "        # idx, targets = get_batch(split='train')\n",
    "        # print(f\"Starting the train step: {step}...\")\n",
    "        # for idx, targets in train_loader:\n",
    "        idx, targets = next(iter(train_loader))\n",
    "        idx = idx.cuda(non_blocking=True)\n",
    "        targets = targets.cuda(non_blocking=True)\n",
    "        logits = model(idx)\n",
    "        batch_size, block_size, embeddings_dims = logits.shape\n",
    "        logits = logits.view(batch_size*block_size, embeddings_dims)\n",
    "        targets = targets.view(batch_size * block_size)\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "\n",
    "        # if device == 0:\n",
    "        #     train_epoch_iterator.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        # print(loss.item())\n",
    "        # break\n",
    "\n",
    "        # if step != 0 and (step % eval_iters == 0 or step == total_steps -1) :\n",
    "        #     loss_values = estimate_loss()\n",
    "        #     print(\"Train Loss at {} steps : {}\".format(step, loss.item()), \"Val Loss at {} steps : {}\".format(step, loss_values['val']))\n",
    "\n",
    "        # Add after a training step:\n",
    "        # unused_params = find_unused_parameters(model)\n",
    "        # print(\"Unused parameters:\", unused_params)\n",
    "        # break\n",
    "\n",
    "\n",
    "    # Cleanup\n",
    "    if device == 0:\n",
    "        wandb.finish()\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "# world_size = torch.cuda.device_count()\n",
    "# print(f\"World size: {world_size}\")\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbZdnSczKzLK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "006ef0c5df0c40f3a8ab486a67e2a27c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "011b5225e1184285be3b637af01aebce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0190bc823cfa4f569673d0e38a0a0a9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04784be0738f4ba69a4b4dd44b931711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "130d4aebf9fe4c5b8119c6647b10342f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14585b2d8e4d490abf54b0166c55fc9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "229667119c3f40f2a01cace6f968ac14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_532dfbe8af8a4f69867b206493355e6e",
      "placeholder": "​",
      "style": "IPY_MODEL_da0f9205f4114382b65b307d54eac834",
      "value": " 102/102 [01:08&lt;00:00,  2.52it/s]"
     }
    },
    "281d510fcef84d999e25c7f1d726cdd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3257101601ef43e9949202e9496b893e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a18e10dd6b94d44ac91610cb6feb453",
      "max": 102,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b198fe21d104ddabe608174c9ddd681",
      "value": 102
     }
    },
    "3b6f04a4e03d4545ba1d030bf130f7e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "456abe4d692b4e068fb0f7c5af26fe55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_57e77de389bf4ca28967421a92db7544",
       "IPY_MODEL_3257101601ef43e9949202e9496b893e",
       "IPY_MODEL_229667119c3f40f2a01cace6f968ac14"
      ],
      "layout": "IPY_MODEL_006ef0c5df0c40f3a8ab486a67e2a27c"
     }
    },
    "4ba9455baefa436d9e79eaabc88d2fdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "532dfbe8af8a4f69867b206493355e6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57e77de389bf4ca28967421a92db7544": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ba9455baefa436d9e79eaabc88d2fdb",
      "placeholder": "​",
      "style": "IPY_MODEL_011b5225e1184285be3b637af01aebce",
      "value": "Loading dataset shards: 100%"
     }
    },
    "5fdef1073deb44f49c75e5a4d4dbe3b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c33bfb3f974f411fa1d20fcdd557052d",
      "max": 11895089,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04784be0738f4ba69a4b4dd44b931711",
      "value": 256000
     }
    },
    "6079efb227a7455a9f76a338c0b52282": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d18c669de02412a959b20eaa818e3a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a932ec56d10b426e974d55e765a9f7c9",
       "IPY_MODEL_feecc9d1f2ee4667beef28b2c5d607d2",
       "IPY_MODEL_ec03a4e8461d4a1880daf1f307e172ff"
      ],
      "layout": "IPY_MODEL_76d88d1671d64fc2afe2e17098045935"
     }
    },
    "769329623b944cb69a2b163a555f3fc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76d88d1671d64fc2afe2e17098045935": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b198fe21d104ddabe608174c9ddd681": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9a18e10dd6b94d44ac91610cb6feb453": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b7d94ba15da4969b13658501c044be5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bf6b831ddf284f97a7d5cbdc251b5546",
       "IPY_MODEL_5fdef1073deb44f49c75e5a4d4dbe3b1",
       "IPY_MODEL_e6970785434b40e5b31690b6c0221a77"
      ],
      "layout": "IPY_MODEL_769329623b944cb69a2b163a555f3fc4"
     }
    },
    "a932ec56d10b426e974d55e765a9f7c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14585b2d8e4d490abf54b0166c55fc9e",
      "placeholder": "​",
      "style": "IPY_MODEL_c574aaeba1c74552b2346f034de152df",
      "value": "Resolving data files: 100%"
     }
    },
    "bf6b831ddf284f97a7d5cbdc251b5546": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_130d4aebf9fe4c5b8119c6647b10342f",
      "placeholder": "​",
      "style": "IPY_MODEL_0190bc823cfa4f569673d0e38a0a0a9e",
      "value": "Map (num_proc=8):   2%"
     }
    },
    "c33bfb3f974f411fa1d20fcdd557052d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c574aaeba1c74552b2346f034de152df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c802c552ec1a40cf9213ca3b11c7af86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6d0c9dace824b5aada1c2af5ec08880": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da0f9205f4114382b65b307d54eac834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6970785434b40e5b31690b6c0221a77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6079efb227a7455a9f76a338c0b52282",
      "placeholder": "​",
      "style": "IPY_MODEL_3b6f04a4e03d4545ba1d030bf130f7e7",
      "value": " 256000/11895089 [02:54&lt;2:00:08, 1614.66 examples/s]"
     }
    },
    "e881780f23164737aabe9ce21d3a1a63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ec03a4e8461d4a1880daf1f307e172ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c802c552ec1a40cf9213ca3b11c7af86",
      "placeholder": "​",
      "style": "IPY_MODEL_281d510fcef84d999e25c7f1d726cdd7",
      "value": " 25868/25868 [00:04&lt;00:00, 6404.35it/s]"
     }
    },
    "feecc9d1f2ee4667beef28b2c5d607d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6d0c9dace824b5aada1c2af5ec08880",
      "max": 25868,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e881780f23164737aabe9ce21d3a1a63",
      "value": 25868
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
